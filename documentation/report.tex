\documentclass{article}
\usepackage{amsmath, amssymb, textcomp}
\usepackage{graphicx}
\usepackage{cite}
\usepackage[hidelinks]{hyperref}
\usepackage[section]{placeins}

\newcommand{\Part}[3][ ]{\ensuremath{\frac{\partial^{#1} #2}{{\partial #3}^{#1}}}}
\newcommand{\Dif}[3][ ]{\ensuremath{\frac{d^{#1} #2}{{d #3}^{#1}}}}
\renewcommand{\O}[1]{\ensuremath{\mathcal{O}\left(#1\right)}}
\renewcommand{\vec}{\bold}

\begin{document}
\title{Plasma Analysis}
\author{Tobias de Jong \& David Kok}
\date{\today}
\maketitle

\section{Introduction}
{\bf Fysische achtergrond, motivatie voor numeriek. Bonuspunten voor gebruik van het woord Topologisch }
We will use CUDA to efficiently calculate the topological properties of magnetic field lines as generated by Chris Smiet's simulations.\cite{PhysRevLett.115.095001}\\
Smiet's work concentrates on plasma's in which the field lines have a non-trivial topological order, such as interlocking rings. Since field lines can not cross, this topological order enhances the stability of the plasma.

The simulations we used as starting data for the analysis in this report do MHD calculations in order to calculate the evolution through time of the plasma, starting out with two or more interlocked rings, evolving into nested tori.
Of this tori we want to calculate properties, foremost the winding number.

To implement the code we were inspired by Marek Fiser.\footnote{\url{http://www.marekfiser.com/Projects/Real-time-visualization-of-3D-vector-field-with-CUDA}} He uses Runge-Kutta integration to visualize (air) flows around a Delta wing, but the idea of integration of a vector field is the same.

\section{Theory and Methods}
\subsection{Theory}
We want to calculate properties of the magnetic field by using field lines associated to the field. In order to do this we need to calculate (finite segments of) field lines. To do this it is enough to note that field lines are the solutions $\vec x(s)$ to the following ordinary differential equation:
\[\Dif{\vec x}{s} = \vec B(\vec x),\]
where $s$ is the parametrization variable.\\
An interesting observation made by \textbf{insertrefhere} is that in an ideal plasma the total helicity of the field, defined through the volume integral:
\[
\text{Helicity} = \int\vec{A}\cdot\vec{B}\ d\vec{x} = \int\vec{A}\cdot (\nabla\times\vec{A})d\vec{x}
\]
is conserved. Therefore the initial value of this integral restricts the possible long-term dynamics of a plasma. In particular we are working with a dataset of highly helical initial data, in which case the field lines will align themselves along the surfaces of nested tori. A line on the surface of a torus can be associated with a winding number, which is the ratio of the winding along the toroidal axis (denoted with $\alpha$) and the winding along the poloidal axis (in our code denoted with $\beta$). To calculate a field line and its winding numbers we will use numerical integration as described in more detail below.


\subsection{Methods}
The goal of our project is to determine topological properties of a pre-computed magnetic field from a magnetohydrodinamical simulation.\\
Our code is split into two main segments. Firstly we numerically integrate the magnetic field to obtain (a numerical approximation of) a field line in the plasma. Secondly we compute several properties of the magnetic field using this field line, such as the length of the line (which estimates the average field strength), the center of the line (estimating the origin of the torus on which it lies) and its winding number, i.e. its contribution to the total helicity of the magnetic field.\\
Both steps can be parallelized heavily. To parallelize the numerical integration of the magnetic field we simply consider more than one field line at a time. In the second step we not only consider multiple field lines at once, but also parallelize the computation of the aforementioned parameters per field line.\\


\section{Numerical theory}
{\bf complexiteit in termen van relevante parameters}
\subsection{Vector field representation}
To represent the vector field $\vec{B}: \mathbb{R}^3 \to \mathbb{R}^3$ in a computer we need to discretize this field. This is done by defining a grid in space and saving the components of the field at each grid point. Note that we are interested in the field lines for fixed moments in time, so all our analysis will not concern the time evolution of the plasma. There are two ways to save all the components: either seperate the components and save a three-dimensional array for each component of the vector field (three $N\times N\times N$ arrays), or save one large array of which each component contains the whole vector at the corresponding gridpoint (one $N\times N\times N$ array of $3D$ vectors).

Since numerical integration algorithms will use all the components of the field at a point at the same time we use the second method. %Or do we? Ik weet eigenlijk niet hoe een 3D texture van binnen werkt...
This choice of representation is both physically intuitive as well as efficient, since it will improve cache coherency and therefore access times.

\subsubsection{Interpolation}
In order to accurately integrate we need very accurate estimates of the original vectorfield at arbitrary locations. It is unfeasable to have such a fine discretization that we have a value for each location of interest. We therefore want to interpolate the vector field to get a good approximation of the value of the vector field at an arbitrary location. The simplest method for interpolation is linear interpolation, for 3D data known as trilinear interpolation.

Interpolation of two- and even three-dimensional data is used in graphics processing in order to map images onto three-dimensional objects. In graphics processing this is known as texturing. Because this problem is very important in graphics processing modern GPU's have efficient texturing units, performing trilinear interpolation in hardware.

Using CUDA enables us to use this hardware accelerated interpolation by storing our vector field data as a texture. Since every RK4-step needs at least 4 vector field fetches this acceleration significantly speeds up the numerical integration. %alhoewel we dit natuurlijk niet gemeten hebben.

\subsection{Runge-Kutta method}
For line integration the fourth order Runge-Kutta method (RK4) was used.  This method is known as a good allround integration method. Compared to standard Forward Euler, RK4 uses four values of the magnetic field to calculate the next point on the field line. This increase in calculation time per step is compensated by a vastly reduced error per step, allowing for a much larger step size $\Delta s$, since RK4 is a fourth order method with total accumulated error $\O{\Delta s^4}$.

\subsection{Algorithm}
To calculate the winding number of a field line passing through a certain starting point we first use RK4 to integrate a large length of field line from the magnetic field data. This process is inherently sequential. Since the interpolation is hardware optimized we are memory access bound in this part of the algorithm, which we have verified by using the NVIDIA profiler. Thus there was nothing to be gained from parallelizing the integration further than one-thread-per-field-line.

To compute the winding number of a field line we need to convert to a toroidal coordinate system. Since not all tori of interest are perfectly aligned this means we first have to find the position and orientation of the torus of interest before we can calculate any other relevant parameters.

By averaging the coordinates over a whole field line we find (a good approximation of) the origin of our torus. This summation can be done efficiently in parallel as a reduction. For implementation we implemented most of the methods described by Harris et al.~\cite{harris2007optimizing}.

Next we need to calculate the orientation of the torus, in particular the normal to the surface plane of the torus. We can find this normal by taking the outer product of the direction of a field line (the $\vec{B}$-field) at a point and the location vector of that point. We perform this local calculation for each point on our field line and implement a similar parallel reduction as before to find the orientation of our torus.

The last part of the torus coordinate system needed is the (center) radius of the torus. For this we calculate the average distance from the origin in the projection onto the plane of the torus. %This is error prone, see discussion.

With this coordinate system calculating the winding numbers of the field lines is as simple as calculating the difference in both poloidal and toroidal coordinates between consecutive data points and summing these along a field line. This is once again implemented as a parallel reduction.

In the implementation it was needed to take care of unwrapping the coordinates modulo $2\pi$ using some inefficient if-statements. %Discussion: One could probably devise a parallel unwrap algorithm, but would it be worth it?
After summing these values we compte the ratio of the total polodoidal and toroidal change, and have found the winding number.

As mentioned before this entire algorithm is parallelized by considering multiple field lines at the same time. In particular we pick a 2D-grid of initial locations for the field lines instead of a single point. Using this data we can show the winding number as a function of position in the nested tori.\\

All analysis of the field lines in our code is done through assigning one thread to each point on the field line, computing a local variable of interest and then integrating/averaging this along a field line. Therefore the execution time of this part of the code is linear in the number of points on our field lines, and therefore linear in the product of the number of field lines we are considering times the number of steps per field line. Since the generating of the field lines is also linear in this product we expect the execution time of our code to be (independently) linear in the number of lines and the number of steps per line. We should remark that there is some fixed runtime which is required to allocate memory, load the magnetic field and initialise the hardware accellerated structures, so even though we expect the runtime to be linear in the number of points we do not expect this line to go through the origin.

\section{Results}
{\bf plaatjes, timings + scaling, portability, meer plaatjes: lines, lengths, windings, masking}
\subsection{Simulated field lines}
Using our code we computed magnetic field lines for given magnetic fields. Included are three examples of the winding numbers computed with this code, and one example of the computed line lengths.


\begin{figure}[!htb]
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{Figures/Rings_Papertwist_twist1_82_steps32k.png}
  \caption{Winding numbers of the field lines of a cross-section through the torus. The vertical axis was scaled since the winding number is almost constant. The vector field used is "Papertwist1-82". The image is $1024\times 1024$ pixels, with $32768$ points per field line.}\label{fig:82-32k}
\endminipage\hfill
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{Figures/Rings_Papertwist_twist1_125_steps8k.png}
  \caption{Winding numbers of the field lines of a cross-section through the torus. The vertical axis was scaled since the winding number is almost constant. The vector field used is "Papertwist1-125". The image is $1024\times 1024$ pixels, with $8196$ points per field line.}\label{fig:125-8k}
\endminipage\hfill
\minipage{0.32\textwidth}%
  \includegraphics[width=\linewidth]{Figures/Rings_Papertwist_twist1_125_steps32k.png}
  \caption{Winding numbers of the field lines of a cross-section through the torus. The vertical axis was scaled since the winding number is almost constant. The vector field used is "Papertwist1-125". The image is $1024\times 1024$ pixels, with $32768$ points per field line.}\label{fig:125-32k}
\endminipage
\end{figure}

In order to make a good image we had to restrict the data in two ways. We firstly changed the colour scale (as mentioned in the caption above) to ensure that the changes in the nearly-constant winding number are visible. Secondly we have discarded (masked) all the points with low total length of the field line, reasoning that the winding numbers of short field lines can be wildly inaccurate. For completeness we include a plot of the length of the field lines and the unmasked (but still colour-clipped) data in figure \ref{fig:125-32k} below.

\begin{figure}[!htb]
\minipage{0.45\textwidth}
  \includegraphics[width=\linewidth]{Figures/Rings_Papertwist_twist1_125_lengths_steps32k.png}
  \caption{Lengths of the field lines of a cross-section through the torus. The vector field used is "Papertwist1-125". The image is $1024\times 1024$ pixels, with $32768$ points per field line.}\label{fig:125-lengths}
\endminipage\hfill
\minipage{0.45\textwidth}
  \includegraphics[width=\linewidth]{Figures/Rings_Papertwist_twist1_125_nomask_steps32k.png}
  \caption{Unmasked winding numbers of the field lines of a cross-section through the torus. The vector field used is "Papertwist1-125". The image is $1024\times 1024$ pixels, with $32768$ points per field line.}\label{fig:125-unmasked}
\endminipage
\end{figure}



\subsection{Speed and scaling}
We have recorded the runtime of the code for the dataset "Shafranov Rot3 ampl 0,05 Animation50", generating and analysing datasets as in figures \ref{fig:82-32k}-\ref{fig:125-32k}. For the first set of timing data we fixed the image size at $1024\times 1024$ pixels (field lines) and varied the number of steps per field line. After that we fixed the number of steps per field line (at $32786$) and varied the number of pixels (field lines) in an image.



\section{Discussion}
{\bf Algoritmekeuzes \textrightarrow betere keuzes, sign flip in center: niet meer nodig?, stitching issues: niet meer nodig?, aannames over geometrie, dynamische stapgrootte, fringes door fouten $(\alpha,\beta)$-afronding.}\\
Because the numerical integration is the only inherently sequential part of the algorithm, we made the choice to minimize calculations during this part of the algorithm. Profiling of the code shows however that it is heavily memory bandwidth bound. This suggest more calculations can be done during integration to improve use of the other computational resources during this part of the algorithm.

\section{Suggestions for future work}

\subsection{Interpolation}
Better algorithms exist for interpolation, most notably cubic interpolation and interpolation schemes which preserve the zero divergence of the magnetic field\cite{McNally01052011}. It is possible that these algorithms could further reduce numerical artefacts. %TODO Which artefacts.
Notably, efficient and open source implementations in CUDA of cubic interpolation exist.\cite{Ruijters01012012}

\subsection{Integration}
As mentioned the code was shown to be largely memory access bound. Implementation of a multi-step method such as Adams-Bashforth could reduce the number of texture fetches per step, while maintaining a high order.\\
Also worth mentioning is that the RK4 integration method uses no special properties of the magentic field, such as the divergencelessness or the Hamiltonian description. It might for example be possible to exploit the knowledge that the system is Hamiltonian by implementing a symplectic method.

\subsection{Optimization}
We remarked before that profiling the code revealed that the algorithm is both memory bound and memory access bound. A possible optimization would be to increase the computational load during the integration of the magnetic field, for example pre-calculating some of the properties required later.



\bibliographystyle{plain}
\bibliography{introduction}
\end{document}
