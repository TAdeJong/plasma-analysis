\documentclass{article}
\usepackage{amsmath, amssymb, textcomp}
\usepackage{graphicx}
\usepackage{cite}
\usepackage[hidelinks]{hyperref}
\usepackage[section]{placeins}

\newcommand{\Part}[3][ ]{\ensuremath{\frac{\partial^{#1} #2}{{\partial #3}^{#1}}}}
\newcommand{\Dif}[3][ ]{\ensuremath{\frac{d^{#1} #2}{{d #3}^{#1}}}}
\renewcommand{\O}[1]{\ensuremath{\mathcal{O}\left(#1\right)}}
\renewcommand{\vec}{\bold}

\begin{document}
\title{Plasma Analysis}
\author{Tobias de Jong \& David Kok}
\date{\today}
\maketitle
\section{Introduction}
{\bf Fysische achtergrond, motivatie voor numeriek. Bonuspunten voor gebruik van het woord Topologisch }
We will use CUDA to efficiently calculate the topological properties of magnetic field lines as generated by Chris Smiet's simulations.\cite{PhysRevLett.115.095001}\\
Smiet's work concentrates on plasma's in which the field lines have a non-trivial topological order, such as interlocking rings. Since field lines can not cross, this topological order enhances the stability of the plasma.

The simulations we used as starting data for the analysis in this report do MHD calculations in order to calculate the evolution through time of the plasma, starting out with two or more interlocked rings, evolving into nested tori.
Of this tori we want to calculate properties, foremost the winding number.

To implement the code we were inspired by Marek Fiser.\footnote{\url{http://www.marekfiser.com/Projects/Real-time-visualization-of-3D-vector-field-with-CUDA}} He uses Runge-Kutta integration to visualize (air) flows around a Delta wing, but the idea of integration of a vector field is the same.
\section{Theory}
{\bf plasma-fluidity, field lines }
\section{Numerical theory}
{\bf RK4, datastructuren (+textures), algoritme ontwerp + complexiteit in termen van relevante parameters, optimalisaties?}
\subsection{Vector field representation}
To represent the vector field $B: \mathbb{R}^3 \to \mathbb{R}^3$ in a computer we need to discretize it. This is done by defining a grid in space and saving the components of the field at each grid point. (We leave time dependance out of the discussion as our calculations are independent for each point in time.) There are two ways to save all the components: either seperate the components and save a three-dimensional array for each component of the vector field, or save one large array of which each component contains the whole vector at the corresponding gridpoint.

We use the latter method as numerical integration algorithms will use all the components of the field at a point at once. %Or do we? Ik weet eigenlijk niet hoe een 3D texture van binnen werkt...
Thus it is both logical, as we can think now still think of the vectors as a single mathematical object, as well as efficient, as this improves cache coherency and access times, to use the latter method.

\subsubsection{Interpolation}
In order to accurately integrate, we need very accurate estimates of the original vectorfield at arbitrary locations. As such it is unfeasable to have such a fine discretization that we have a value for each location. 
Thus we want to interpolate the vector field, in order to get a good approximation of value of the vector field at an arbitrary location. The simplest method for interpolation is linear interpolation, for 3D data known as trilinear interpolation.

Interpolation of two- and even three dimenional data is used in graphics processing in order to map images onto three-dimensional objects and is known as texturing. This means that GPU's have efficient texturing units, hardware accelerating trilinear interpolation. 

Using CUDA enables us to use this hardware accelerated interpolation by storing our vector field data as a texture. Since every RK4-step needs at least 6 data points, this is a large benefit. %alhoewel we dit natuurlijk niet gemeten hebben.
\subsection{Algorithm}
To calculate the windingnumber of a field line passing through a certain starting point, we first use RK4 to integrate a large length of field line. This process is inherently sequential and since the interpolation is hardware optimized, we are memory access bound in this part of the algorithm. This was verified by using the NVIDIA profiler. Thus there was no use in parallizing the integration further than one-thread-per-field-line.

Since the torus moves through space, we first need to find its position and orientation before we can calculate any other relevant parameters.
By taking the average coordinate over the whole field line, we find the origin of our torus. This summation can be done efficiently in parallel as a reduction. For implementation we implemented most of the methods described by Harris et al.~\cite{harris2007optimizing}

Next we need to calculate the orientation of the torus. This again is an average of a local calculation on the field line and is implemented as a similar parallel reduction.

The last part of the torus coordinate system needed is the radius of the torus. For this we calculate the average distance from the origin in the projection onto the $xy$-plane. %This is error prone, see discussion.

With this coordinate system, calculating the winding numbers of the field lines is as simple as calculating the difference in both polloidal and toroidal coordinates between consecutive data points and summing these. This is once again implemented as a parallel reduction. In the implementation it was needed to take care of unwrapping the coordinates modulo $2\pi$ using some inefficient if-statements. %Discussion: One could probably devise a parallel unwrap algorithm, but would it be worth it?
Taking the ratio of the total pollodoidal and toroidal change, we have found the winding number.

Taking a 2D-grid of winding numbers and doing above calculations in parallel, we can show the winding number as a function of position in the torus.
\section{Results}
{\bf plaatjes, timings + scaling, portability, meer plaatjes: lines, lengths, windings, masking}
\section{Discussion}
{\bf Algoritmekeuzes \textrightarrow betere keuzes, sign flip in center, stitching issues, aannames over geometrie}
\section{Suggestions for future work}
{\bf betere algoritmes: vanwege memory bound meer rekenen tijdens RK4, normaal implementeren, betere modulariteit, dynamische stapgrootte. Variantie/FFT op windingsgetallen in tijdsdomein}
Better algorithms exist for interpolation, most notably cubic interpolation and interpolation schemes which preserve the zero divergence of the magnetic field\cite{McNally01052011}. It is possible that these algorithms could further reduce numerical artefacts. %TODO Which artefacts.
Notably, efficient, open source, implementations in CUDA of cubic interpolation exist.\cite{Ruijters01012012}
\bibliographystyle{plain}
\bibliography{introduction}
\end{document}
